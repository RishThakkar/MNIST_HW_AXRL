{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae9f783d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from keras.utils import to_categorical\n",
    "from skimage.transform import resize\n",
    "import h5py\n",
    "import os\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41bb695d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42, 42, 1)\n"
     ]
    }
   ],
   "source": [
    "# # Load MNIST dataset\n",
    "# (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# # Function to resize and normalize each image by its max pixel value\n",
    "# def resize_and_normalize(images, new_size=(42, 42)):\n",
    "#     resized_images = np.zeros((images.shape[0], new_size[0], new_size[1]))\n",
    "#     for i in range(images.shape[0]):\n",
    "#         resized = resize(images[i], new_size, mode='reflect', anti_aliasing=True)\n",
    "#         max_pixel_value = np.max(resized)\n",
    "#         if max_pixel_value > 0:  # Avoid division by zero\n",
    "#             resized_images[i] = resized / max_pixel_value\n",
    "#         else:\n",
    "#             resized_images[i] = resized  # If max is 0, just use the resized image\n",
    "#     return resized_images\n",
    "\n",
    "# # Resize and normalize images\n",
    "# x_train_resized = resize_and_normalize(x_train)\n",
    "# x_test_resized = resize_and_normalize(x_test)\n",
    "\n",
    "# # Reshape to add channel dimension (required for Keras)\n",
    "# x_train_resized = x_train_resized.reshape(-1, 42, 42, 1)\n",
    "# x_test_resized = x_test_resized.reshape(-1, 42, 42, 1)\n",
    "\n",
    "# # One-hot encode labels\n",
    "# y_train = to_categorical(y_train)\n",
    "# y_test = to_categorical(y_test)\n",
    "\n",
    "# print(x_test_resized[0].shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c655504b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42, 42, 1)\n"
     ]
    }
   ],
   "source": [
    "# # Load MNIST dataset\n",
    "# (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# # Resize images to 42x42\n",
    "# x_train_resized = np.zeros((x_train.shape[0], 42, 42))\n",
    "# for i in range(x_train.shape[0]):\n",
    "#     x_train_resized[i] = resize(x_train[i], (42, 42))\n",
    "\n",
    "# x_test_resized = np.zeros((x_test.shape[0], 42, 42))\n",
    "# for i in range(x_test.shape[0]):\n",
    "#     x_test_resized[i] = resize(x_test[i], (42, 42))\n",
    "\n",
    "# # Reshape to add channel dimension (required for Keras)\n",
    "# x_train_resized = x_train_resized.reshape(-1, 42, 42, 1)\n",
    "# x_test_resized = x_test_resized.reshape(-1, 42, 42, 1)\n",
    "\n",
    "# # Normalize pixel values to range [0, 1]\n",
    "# x_train_resized = x_train_resized.astype('float32') / 255.0\n",
    "# x_test_resized = x_test_resized.astype('float32') / 255.0\n",
    "# print(x_test_resized[0].shape)\n",
    "\n",
    "# # One-hot encode labels\n",
    "# y_train = to_categorical(y_train)\n",
    "# y_test = to_categorical(y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3dc956",
   "metadata": {},
   "source": [
    "# Resize as 42x42 but does not normalize, and make it between 0 and 127, so that it can be represented in 7 bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb95bfc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127\n",
      "(10000, 42, 42, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Resize images to 42x42 using OpenCV\n",
    "x_train_resized = np.zeros((x_train.shape[0], 42, 42), dtype=np.uint8)\n",
    "for i in range(x_train.shape[0]):\n",
    "    x_train_resized[i] = cv2.resize(x_train[i], (42, 42))\n",
    "\n",
    "x_test_resized = np.zeros((x_test.shape[0], 42, 42), dtype=np.uint8)\n",
    "for i in range(x_test.shape[0]):\n",
    "    x_test_resized[i] = cv2.resize(x_test[i], (42, 42))\n",
    "\n",
    "# Reshape to add channel dimension (required for Keras)\n",
    "x_train_resized = x_train_resized.reshape(-1, 42, 42, 1)\n",
    "x_test_resized = x_test_resized.reshape(-1, 42, 42, 1)\n",
    "\n",
    "for i in range(x_train.shape[0]):\n",
    "    x_train_resized[i] = x_train_resized[i]/2\n",
    "\n",
    "for i in range(x_test.shape[0]):\n",
    "    x_test_resized[i] = x_test_resized[i]/2\n",
    "\n",
    "x_test_max = np.max(x_test_resized)\n",
    "print(x_test_max)\n",
    "print(x_test_resized.shape)\n",
    "\n",
    "# One-hot encode labels\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c287349e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(x_test.shape)\n",
    "# print(x_test_resized.shape)\n",
    "# for row in x_test_resized[0]:\n",
    "#     print(' '.join(map(lambda pixel: '{:3}'.format(float(pixel)), row)))\n",
    "\n",
    "# for row in x_test[0]:\n",
    "    # print(' '.join(map(lambda pixel: '{:3}'.format(float(pixel)), row)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e1d0bd",
   "metadata": {},
   "source": [
    "### Convert into bits, and then convert again in Floating point but using our Fixed point Representation: 127 -> 7.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5529566",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_binary(images):\n",
    "    binary_images = []\n",
    "\n",
    "    for img in images:\n",
    "        binary_img = []\n",
    "\n",
    "        for pixel_value in np.nditer(img):\n",
    "            binary_str = format(pixel_value, '08b')\n",
    "            binary_img.append(binary_str)\n",
    "\n",
    "        binary_images.append(binary_img)\n",
    "\n",
    "    return binary_images\n",
    "\n",
    "# Convert x_test_resized and x_train_resized to binary\n",
    "binary_x_test_resized = convert_to_binary(x_test_resized)\n",
    "binary_x_train_resized = convert_to_binary(x_train_resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8ed2bb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_to_float(binary_str):\n",
    "    weights = [-8, 4, 2, 1, 0.5, 0.25, 0.125, 0.0625]\n",
    "    \n",
    "    result = 0\n",
    "    \n",
    "    for i, bit in enumerate(binary_str):\n",
    "        result += int(bit) * weights[i]\n",
    "    \n",
    "    return result\n",
    "\n",
    "def convert_to_float(binary_images):\n",
    "    float_images = []\n",
    "\n",
    "    for img in binary_images:\n",
    "        float_img = []\n",
    "\n",
    "        for binary_str in img:\n",
    "            float_value = binary_to_float(binary_str)\n",
    "            float_img.append(float_value)\n",
    "        \n",
    "        float_img = np.array(float_img).reshape((42, 42))\n",
    "        float_images.append(float_img)\n",
    "\n",
    "    return np.array(float_images)\n",
    "\n",
    "# Convert binary_x_test_resized and binary_x_train_resized to floating point values as numpy arrays\n",
    "FP_x_test_resized = convert_to_float(binary_x_test_resized)\n",
    "FP_x_train_resized = convert_to_float(binary_x_train_resized)\n",
    "\n",
    "FP_x_train_resized = FP_x_train_resized.reshape(-1, 42, 42, 1)\n",
    "FP_x_test_resized = FP_x_test_resized.reshape(-1, 42, 42, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "47be5629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.9375\n",
      "(10000, 42, 42, 1)\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.1875 1.0625 2.0625 2.8125 2.5625 2.4375 2.125 1.1875 0.75 0.4375 0.0625 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5625 2.75 4.75 6.0 5.5625 5.3125 4.8125 3.25 2.5 2.0 1.1875 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.875 0.5625 0.1875 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 5.1875 6.875 7.5625 7.4375 7.375 7.3125 7.0 6.6875 6.25 5.375 5.125 5.125 5.125 5.125 5.125 5.125 5.125 5.125 5.125 5.125 5.0 4.5625 2.875 1.125 0.1875 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.75 3.75 5.125 5.625 5.1875 5.4375 5.875 6.375 6.875 7.25 7.0625 6.8125 6.6875 7.0 7.0625 7.0625 7.0625 7.0 6.9375 6.6875 6.875 7.0 6.6875 4.8125 2.5 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.25 1.4375 2.3125 2.75 2.0625 2.4375 3.1875 4.0 5.125 6.125 6.75 6.4375 6.0625 6.75 6.9375 6.9375 6.9375 6.875 6.6875 6.1875 6.9375 7.8125 7.875 6.0625 3.5 0.6875 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0625 0.25 0.4375 0.5625 0.375 0.4375 0.625 0.8125 1.1875 1.8125 2.8125 2.25 1.75 2.8125 3.0625 3.0625 3.0625 2.9375 2.625 1.875 4.5625 7.5 7.8125 5.6875 2.875 0.5625 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.125 0.375 0.875 0.625 0.3125 0.875 1.0625 1.0625 1.0625 1.0 1.0 1.5 4.625 7.5625 7.3125 4.5625 1.625 0.3125 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0625 0.625 2.8125 5.625 7.5625 6.1875 3.125 0.375 0.0625 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.25 1.5625 5.5 7.1875 7.125 4.0 1.625 0.0625 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.1875 3.1875 6.6875 7.625 6.75 2.9375 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0625 0.25 2.4375 5.1875 7.375 7.1875 5.625 2.0 0.5625 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.25 1.25 4.3125 7.25 7.8125 5.375 2.375 0.625 0.125 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.5 2.5 5.4375 7.6875 7.0625 3.9375 0.875 0.125 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.75 3.75 6.1875 7.4375 5.625 2.625 0.0625 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.125 1.1875 5.0625 6.875 6.875 3.3125 1.25 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0625 2.9375 6.3125 6.9375 5.75 1.875 0.4375 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0625 0.3125 2.5 5.125 7.3125 6.4375 4.1875 0.8125 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.3125 1.625 4.5625 7.25 7.5 5.0 2.0 0.375 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.125 1.0 3.875 6.25 7.625 6.5625 3.625 0.75 0.125 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.75 2.4375 6.125 7.375 7.1875 5.0 2.25 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0625 2.6875 5.75 7.375 7.375 6.125 2.625 0.875 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0625 0.5 3.875 7.25 7.8125 6.25 3.9375 1.1875 0.25 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0625 0.5 1.8125 5.0 7.8125 7.3125 4.5625 1.625 0.3125 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.375 1.6875 5.125 6.9375 7.3125 4.875 2.375 0.3125 0.0625 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.25 3.375 6.625 7.6875 7.0 3.5 1.3125 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0625 0.25 2.5 5.25 7.375 7.9375 6.875 2.6875 0.8125 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.25 1.3125 4.25 7.125 7.75 7.9375 6.875 2.6875 0.8125 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.4375 2.375 5.3125 7.75 7.875 7.625 6.375 2.4375 0.6875 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.625 3.125 5.8125 7.875 7.6875 6.75 5.0 1.8125 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.625 3.125 5.8125 7.6875 6.875 4.125 1.375 0.4375 0.0625 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.3125 1.5625 2.9375 3.8125 3.3125 1.75 0.25 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rishit Thakkar\\AppData\\Local\\Temp\\ipykernel_11392\\3202260793.py:5: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  print(' '.join(map(lambda pixel: '{:3}'.format(float(pixel)), row)))\n"
     ]
    }
   ],
   "source": [
    "max_val = np.max(FP_x_test_resized)\n",
    "print(max_val)\n",
    "print(FP_x_test_resized.shape)\n",
    "for row in FP_x_test_resized[0]:\n",
    "    print(' '.join(map(lambda pixel: '{:3}'.format(float(pixel)), row)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "22512bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Rishit Thakkar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.6192 - loss: 1.1395 - val_accuracy: 0.9409 - val_loss: 0.2000\n",
      "Epoch 2/15\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9005 - loss: 0.3149 - val_accuracy: 0.9603 - val_loss: 0.1343\n",
      "Epoch 3/15\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9229 - loss: 0.2459 - val_accuracy: 0.9682 - val_loss: 0.1109\n",
      "Epoch 4/15\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9363 - loss: 0.2052 - val_accuracy: 0.9685 - val_loss: 0.1057\n",
      "Epoch 5/15\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9388 - loss: 0.1945 - val_accuracy: 0.9736 - val_loss: 0.0873\n",
      "Epoch 6/15\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9406 - loss: 0.1894 - val_accuracy: 0.9725 - val_loss: 0.0880\n",
      "Epoch 7/15\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9442 - loss: 0.1792 - val_accuracy: 0.9760 - val_loss: 0.0792\n",
      "Epoch 8/15\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9479 - loss: 0.1688 - val_accuracy: 0.9753 - val_loss: 0.0777\n",
      "Epoch 9/15\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9465 - loss: 0.1711 - val_accuracy: 0.9748 - val_loss: 0.0790\n",
      "Epoch 10/15\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9478 - loss: 0.1677 - val_accuracy: 0.9753 - val_loss: 0.0762\n",
      "Epoch 11/15\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9485 - loss: 0.1692 - val_accuracy: 0.9757 - val_loss: 0.0757\n",
      "Epoch 12/15\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9484 - loss: 0.1601 - val_accuracy: 0.9760 - val_loss: 0.0731\n",
      "Epoch 13/15\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9490 - loss: 0.1633 - val_accuracy: 0.9728 - val_loss: 0.0815\n",
      "Epoch 14/15\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9502 - loss: 0.1579 - val_accuracy: 0.9776 - val_loss: 0.0746\n",
      "Epoch 15/15\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9494 - loss: 0.1599 - val_accuracy: 0.9771 - val_loss: 0.0719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dropout, Dense\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(4, kernel_size=(3, 3), activation='relu', input_shape=(42, 42, 1)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(4, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(FP_x_train_resized, y_train, batch_size=50, epochs=15, validation_data=(FP_x_test_resized, y_test))\n",
    "\n",
    "model.save('model_weights_FP.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e82956f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights saved to model_weights_FP.txt\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# Load the model from the .h5 file\n",
    "model = load_model('model_weights_FP.h5')\n",
    "\n",
    "# Get the weights for each layer\n",
    "weights = model.get_weights()\n",
    "\n",
    "# Save the weights to a text file\n",
    "with open('model_weights_FP.txt', 'w') as file:\n",
    "    for layer_weights in weights:\n",
    "        for weight in layer_weights.flatten():\n",
    "            file.write(str(weight) + '\\n')\n",
    "\n",
    "print(\"Model weights saved to model_weights_FP.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2aced222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ee72011c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 859us/step - accuracy: 0.9707 - loss: 0.0880\n",
      "Test Loss: 0.07181327790021896\n",
      "Test Accuracy: 0.9771000146865845\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dropout, Dense\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "model = load_model('model_weights_FP.h5')\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(FP_x_test_resized, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66209071",
   "metadata": {},
   "source": [
    "### Weights Rearrange for our dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9896a896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.42765197  0.08138641  0.01048726 ... -0.08735301 -0.49302652\n",
      "  0.24226427]\n",
      "Model weights saved to model_weights_FP.txt\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "text_file = np.loadtxt(\"model_weights_FP.txt\")\n",
    "# print(text_file[188:3428])\n",
    "\n",
    "array_b = np.zeros(3240)\n",
    "array_a = text_file[188:3428]\n",
    "print(array_a)\n",
    "\n",
    "for i in range(10):\n",
    "    for j in range(4):\n",
    "        for k in range(9*9):\n",
    "            array_b[i + k * 10 + (j * 9 * 9 * 10)] = array_a[(j * 10) + (k * 40) + i]\n",
    "\n",
    "array_final = np.zeros(3438)\n",
    "\n",
    "array_final[0:188] = text_file[0:188]\n",
    "array_final[188:3428] = array_b\n",
    "array_final[3428:3438] = text_file[3428:3438]\n",
    "\n",
    "\n",
    "with open('model_weights_FP_rearrange.txt', 'w') as file:\n",
    "    for layer_weights in array_final:\n",
    "            file.write(str(layer_weights) + '\\n')\n",
    "\n",
    "print(\"Model weights saved to model_weights_FP.txt\")\n",
    "\n",
    "\n",
    "np.savetxt(\"model_weights_FP_rearrange.txt\", array_final, delimiter=',', fmt='%f')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8b90702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Predicted class probabilities: [[0.04511153 0.04166789 0.08247737 0.20467307 0.02355923 0.2535915\n",
      "  0.02111134 0.03081674 0.04968829 0.24730305]]\n",
      "Predicted class label: 5\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "# Load and preprocess the single image\n",
    "img_path = 'demo1_normalized.tiff'  \n",
    "img = image.load_img(img_path, target_size=(42, 42), color_mode='grayscale')  \n",
    "img_array = image.img_to_array(img)\n",
    "img_array = np.expand_dims(img_array, axis=0)\n",
    "# img_array /= 255.  # Normalize the pixel values\n",
    "\n",
    "# Predict the output of the model for the single image\n",
    "predictions = model.predict(img_array)\n",
    "\n",
    "# Print the predicted class probabilities\n",
    "print(\"Predicted class probabilities:\", predictions)\n",
    "\n",
    "# Get the predicted class label\n",
    "predicted_class = np.argmax(predictions)\n",
    "print(\"Predicted class label:\", predicted_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671ee045",
   "metadata": {},
   "source": [
    "# Conv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca4f15c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights_and_biases = np.loadtxt(\"model_weights.txt\")\n",
    "\n",
    "# weights_shape = (3, 3, 1, 4)  # Assuming kernel size of (3, 3) and 4 filters\n",
    "\n",
    "# weights = weights_and_biases[0:36].reshape(weights_shape)  # weights\n",
    "# biases = weights_and_biases[36:40]  # biases\n",
    "\n",
    "\n",
    "# #Model Below\n",
    "# model = Sequential()\n",
    "# model.add(Conv2D(4, kernel_size=(3, 3), activation='relu', input_shape=(42, 42, 1)))\n",
    "\n",
    "# model.layers[0].set_weights([weights, biases])\n",
    "\n",
    "\n",
    "\n",
    "# img_path = 'demo7_binary.png'  \n",
    "# img = image.load_img(img_path, target_size=(42, 42), color_mode='grayscale')  \n",
    "# img_array = image.img_to_array(img)\n",
    "# img_array = np.expand_dims(img_array, axis=0)\n",
    "# # img_array /= 255.  \n",
    "# img_flat = img_array.flatten()\n",
    "# img_array = np.array(img_flat)\n",
    "\n",
    "\n",
    "# img_array = img_flat.reshape((1, 42, 42, 1))  # Reshape to (batch_size, height, width, channels)\n",
    "\n",
    "# conv1_output = model.predict(img_array)\n",
    "\n",
    "# print(\"Output shape of the first convolutional layer:\", conv1_output.shape)\n",
    "\n",
    "# # Save the convolutional layer output\n",
    "# with open('Python_prog_out_conv.txt', 'w') as file:\n",
    "#     for c in range(conv1_output.shape[3]):  \n",
    "#         file.write(\"Channel %d:\\n\" % (c + 1))\n",
    "\n",
    "#         for i in range(conv1_output.shape[1]):  \n",
    "#             for j in range(conv1_output.shape[2]):  \n",
    "#                 file.write(\"%0.2f\" % conv1_output[0, i, j, c])\n",
    "#                 if j != conv1_output.shape[2] - 1:\n",
    "#                     file.write(\", \")\n",
    "#             file.write(\"\\n\")\n",
    "\n",
    "#         file.write(\"\\n\")\n",
    "\n",
    "# print(\"Convolutional layer output saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3016dfe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Output shape of the first convolutional layer: (1, 40, 40, 4)\n",
      "Convolutional layer output saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Rishit Thakkar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "\n",
    "# Load weights and biases from a text file\n",
    "weights_and_biases = np.loadtxt(\"model_weights.txt\")\n",
    "\n",
    "# Assume the weights shape as per the convolutional kernel and filters\n",
    "weights_shape = (3, 3, 1, 4)  # Kernel size (3, 3), 4 filters\n",
    "weights = weights_and_biases[0:36].reshape(weights_shape)\n",
    "biases = weights_and_biases[36:40]\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(4, kernel_size=(3, 3), activation='relu', input_shape=(42, 42, 1)))\n",
    "model.layers[0].set_weights([weights, biases])\n",
    "\n",
    "# Load the pixel values from a text file\n",
    "img_array = np.loadtxt('mnist_image_1.txt')  \n",
    "\n",
    "# Reshape the array to match the input shape of the model\n",
    "img_array = img_array.reshape((1, 42, 42, 1))  # (batch_size, height, width, channels)\n",
    "\n",
    "# Make predictions using the model\n",
    "conv1_output = model.predict(img_array)\n",
    "\n",
    "# Print output shape of the first convolutional layer\n",
    "print(\"Output shape of the first convolutional layer:\", conv1_output.shape)\n",
    "\n",
    "# Save the convolutional layer output\n",
    "with open('Python_prog_out_conv.txt', 'w') as file:\n",
    "    for c in range(conv1_output.shape[3]):\n",
    "        file.write(\"Channel %d:\\n\" % (c + 1))\n",
    "        for i in range(conv1_output.shape[1]):\n",
    "            for j in range(conv1_output.shape[2]):\n",
    "                file.write(\"%0.2f\" % conv1_output[0, i, j, c])\n",
    "                if j != conv1_output.shape[2] - 1:\n",
    "                    file.write(\", \")\n",
    "            file.write(\"\\n\")\n",
    "        file.write(\"\\n\")\n",
    "\n",
    "print(\"Convolutional layer output saved\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d907c86f",
   "metadata": {},
   "source": [
    "# Max1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfb07cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Output shape of the first Max Pool layer: (1, 20, 20, 4)\n",
      "Max 1 layer output saved\n"
     ]
    }
   ],
   "source": [
    "# Load the weights and biases\n",
    "weights_and_biases = np.loadtxt(\"model_weights.txt\")\n",
    "weights_shape = (3, 3, 1, 4)  # Kernel size of (3, 3) and 4 filters\n",
    "weights = weights_and_biases[0:36].reshape(weights_shape)  # Weights\n",
    "biases = weights_and_biases[36:40]  # Biases\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(4, kernel_size=(3, 3), activation='relu', input_shape=(42, 42, 1)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.layers[0].set_weights([weights, biases])\n",
    "\n",
    "# Load pixel values from a text file\n",
    "img_array = np.loadtxt('mnist_image_1.txt') \n",
    "img_array = img_array.reshape((1, 42, 42, 1))  # Reshape for the model\n",
    "\n",
    "# Predict using the model\n",
    "output = model.predict(img_array)\n",
    "\n",
    "# Output the shape of the result after Max Pooling layer\n",
    "print(\"Output shape of the first Max Pool layer:\", output.shape)\n",
    "\n",
    "# Save the output of the Max Pooling layer\n",
    "with open('Python_prog_out_Max1.txt', 'w') as file:\n",
    "    for c in range(output.shape[3]):\n",
    "        file.write(\"Channel %d:\\n\" % (c + 1))\n",
    "        for i in range(output.shape[1]):\n",
    "            for j in range(output.shape[2]):\n",
    "                file.write(\"%0.2f\" % output[0, i, j, c])\n",
    "                if j != output.shape[2] - 1:\n",
    "                    file.write(\", \")\n",
    "            file.write(\"\\n\")\n",
    "        file.write(\"\\n\")\n",
    "\n",
    "print(\"Max 1 layer output saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdefbaf1",
   "metadata": {},
   "source": [
    "# Conv 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fde0b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Output shape of the Second convolutional layer: (1, 18, 18, 4)\n",
      "Convolutional layer output saved\n"
     ]
    }
   ],
   "source": [
    "weights_and_biases = np.loadtxt(\"model_weights.txt\")\n",
    "\n",
    "weights_shape = (3, 3, 1, 4)  # Assuming kernel size of (3, 3) and 4 filters\n",
    "weights2_shape = (3, 3, 4, 4)\n",
    "\n",
    "weights = weights_and_biases[0:36].reshape(weights_shape)  # weights\n",
    "biases = weights_and_biases[36:40]  # biases\n",
    "\n",
    "weights2 = weights_and_biases[40:184].reshape(weights2_shape)\n",
    "biases2 = weights_and_biases[184:188]\n",
    "\n",
    "#Model Below\n",
    "model = Sequential()\n",
    "model.add(Conv2D(4, kernel_size=(3, 3), activation='relu', input_shape=(42, 42, 1)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(4, kernel_size=(3, 3), activation='relu'))\n",
    "\n",
    "model.layers[0].set_weights([weights, biases])\n",
    "model.layers[2].set_weights([weights2, biases2])\n",
    "\n",
    "\n",
    "\n",
    "# Load the pixel values from a text file\n",
    "img_array = np.loadtxt('mnist_image_1.txt')  \n",
    "\n",
    "# Reshape the array to match the input shape of the model\n",
    "img_array = img_array.reshape((1, 42, 42, 1))  # (batch_size, height, width, channels)\n",
    "\n",
    "conv1_output = model.predict(img_array)\n",
    "\n",
    "print(\"Output shape of the Second convolutional layer:\", conv1_output.shape)\n",
    "\n",
    "# Save the convolutional layer output\n",
    "with open('Python_prog_out_conv2.txt', 'w') as file:\n",
    "    for c in range(conv1_output.shape[3]):  \n",
    "        file.write(\"Channel %d:\\n\" % (c + 1))\n",
    "\n",
    "        for i in range(conv1_output.shape[1]):  \n",
    "            for j in range(conv1_output.shape[2]):  \n",
    "                file.write(\"%0.2f\" % conv1_output[0, i, j, c])\n",
    "                if j != conv1_output.shape[2] - 1:\n",
    "                    file.write(\", \")\n",
    "            file.write(\"\\n\")\n",
    "\n",
    "        file.write(\"\\n\")\n",
    "\n",
    "print(\"Convolutional layer output saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acb6353",
   "metadata": {},
   "source": [
    "# Max 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9931121e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Output shape of the 2nd MaxPool layer: (1, 9, 9, 4)\n",
      "Maxpool layer 2 output saved\n"
     ]
    }
   ],
   "source": [
    "weights_and_biases = np.loadtxt(\"model_weights.txt\")\n",
    "\n",
    "weights_shape = (3, 3, 1, 4)  # Assuming kernel size of (3, 3) and 4 filters\n",
    "weights2_shape = (3, 3, 4, 4)\n",
    "\n",
    "weights = weights_and_biases[0:36].reshape(weights_shape)  # weights\n",
    "biases = weights_and_biases[36:40]  # biases\n",
    "\n",
    "weights2 = weights_and_biases[40:184].reshape(weights2_shape)\n",
    "biases2 = weights_and_biases[184:188]\n",
    "\n",
    "#Model Below\n",
    "model = Sequential()\n",
    "model.add(Conv2D(4, kernel_size=(3, 3), activation='relu', input_shape=(42, 42, 1)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(4, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "model.layers[0].set_weights([weights, biases])\n",
    "model.layers[2].set_weights([weights2, biases2])\n",
    "\n",
    "img_array = np.loadtxt('mnist_image_1.txt')\n",
    "\n",
    "img_array = img_array.reshape((1, 42, 42, 1))  # Reshape to (batch_size, height, width, channels)\n",
    "\n",
    "conv1_output = model.predict(img_array)\n",
    "\n",
    "print(\"Output shape of the 2nd MaxPool layer:\", conv1_output.shape)\n",
    "\n",
    "# Save the convolutional layer output\n",
    "with open('Python_prog_out_max2.txt', 'w') as file:\n",
    "    for c in range(conv1_output.shape[3]):  \n",
    "        file.write(\"Channel %d:\\n\" % (c + 1))\n",
    "\n",
    "        for i in range(conv1_output.shape[1]):  \n",
    "            for j in range(conv1_output.shape[2]):  \n",
    "                file.write(\"%0.2f\" % conv1_output[0, i, j, c])\n",
    "                if j != conv1_output.shape[2] - 1:\n",
    "                    file.write(\", \")\n",
    "            file.write(\"\\n\")\n",
    "\n",
    "        file.write(\"\\n\")\n",
    "\n",
    "print(\"Maxpool layer 2 output saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980f6bb2",
   "metadata": {},
   "source": [
    "# Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc77e080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x00000185819A7E20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x00000185819A7E20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Output shape of the Flatten layer: (1, 324)\n",
      "Flatten layer output saved\n"
     ]
    }
   ],
   "source": [
    "weights_and_biases = np.loadtxt(\"model_weights.txt\")\n",
    "\n",
    "weights_shape = (3, 3, 1, 4)  # Assuming kernel size of (3, 3) and 4 filters\n",
    "weights2_shape = (3, 3, 4, 4)\n",
    "weights3_shape = (324, 10)\n",
    "\n",
    "weights = weights_and_biases[0:36].reshape(weights_shape)  # weights\n",
    "biases = weights_and_biases[36:40]  # biases\n",
    "\n",
    "weights2 = weights_and_biases[40:184].reshape(weights2_shape)\n",
    "biases2 = weights_and_biases[184:188]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Model Below\n",
    "model = Sequential()\n",
    "model.add(Conv2D(4, kernel_size=(3, 3), activation='relu', input_shape=(42, 42, 1)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(4, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "model.add(Flatten())\n",
    "\n",
    "\n",
    "model.layers[0].set_weights([weights, biases])\n",
    "model.layers[2].set_weights([weights2, biases2])\n",
    "\n",
    "\n",
    "img_array = np.loadtxt('mnist_image_1.txt')\n",
    "\n",
    "img_array = img_array.reshape((1, 42, 42, 1))  # Reshape to (batch_size, height, width, channels)\n",
    "\n",
    "conv1_output = model.predict(img_array)\n",
    "\n",
    "# Save the convolutional layer output\n",
    "print(\"Output shape of the Flatten layer:\", conv1_output.shape)\n",
    "\n",
    "# Save the flatten layer output\n",
    "with open('Python_prog_out_Flatten.txt', 'w') as file:\n",
    "    for i in range(conv1_output.shape[1]):\n",
    "        file.write(\"Neuron %d: %0.2f\\n\" % (i + 1, conv1_output[0, i]))\n",
    "\n",
    "print(\"Flatten layer output saved\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59abb0ad",
   "metadata": {},
   "source": [
    "# Flatten and Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2f8aaf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000018582E34D60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000018582E34D60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Output shape of the Dense layer: (1, 10)\n",
      "Dense layer output saved\n"
     ]
    }
   ],
   "source": [
    "weights_and_biases = np.loadtxt(\"model_weights.txt\")\n",
    "\n",
    "weights_shape = (3, 3, 1, 4)  # Assuming kernel size of (3, 3) and 4 filters\n",
    "weights2_shape = (3, 3, 4, 4)\n",
    "weights3_shape = (324, 10)\n",
    "\n",
    "weights = weights_and_biases[0:36].reshape(weights_shape)  # weights\n",
    "biases = weights_and_biases[36:40]  # biases\n",
    "\n",
    "weights2 = weights_and_biases[40:184].reshape(weights2_shape)\n",
    "biases2 = weights_and_biases[184:188]\n",
    "\n",
    "weights3 = weights_and_biases[188:3428].reshape(weights3_shape)\n",
    "biases3 = weights_and_biases[3428:3438]\n",
    "\n",
    "#Model Below\n",
    "model = Sequential()\n",
    "model.add(Conv2D(4, kernel_size=(3, 3), activation='relu', input_shape=(42, 42, 1)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(4, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation='relu'))\n",
    "\n",
    "model.layers[0].set_weights([weights, biases])\n",
    "model.layers[2].set_weights([weights2, biases2])\n",
    "model.layers[5].set_weights([weights3, biases3])\n",
    "\n",
    "\n",
    "img_array = np.loadtxt('mnist_image_1.txt')\n",
    "img_array = img_array.reshape((1, 42, 42, 1))  # Reshape to (batch_size, height, width, channels)\n",
    "\n",
    "conv1_output = model.predict(img_array)\n",
    "\n",
    "# Save the convolutional layer output\n",
    "print(\"Output shape of the Dense layer:\", conv1_output.shape)\n",
    "\n",
    "# Save the dense layer output\n",
    "with open('Python_prog_out_Dense.txt', 'w') as file:\n",
    "    file.write(\"Channel 1: \\n\")\n",
    "    for i in range(conv1_output.shape[1]):\n",
    "        file.write(\"%0.2f\\n\" % (conv1_output[0, i]))\n",
    "\n",
    "print(\"Dense layer output saved\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae63e5d",
   "metadata": {},
   "source": [
    "# Check with quantized weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f75e324e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 929us/step - accuracy: 0.8716 - loss: nan\n",
      "Test Loss: nan\n",
      "Test Accuracy: 0.8787999749183655\n"
     ]
    }
   ],
   "source": [
    "weights_and_biases = np.loadtxt(\"model_weights.txt\")\n",
    "\n",
    "weights_shape = (3, 3, 1, 4)  # Assuming kernel size of (3, 3) and 4 filters\n",
    "weights2_shape = (3, 3, 4, 4)\n",
    "weights3_shape = (324, 10)\n",
    "\n",
    "weights = weights_and_biases[0:36].reshape(weights_shape)  # weights\n",
    "biases = weights_and_biases[36:40]  # biases\n",
    "\n",
    "weights2 = weights_and_biases[40:184].reshape(weights2_shape)\n",
    "biases2 = weights_and_biases[184:188]\n",
    "\n",
    "weights3 = weights_and_biases[188:3428].reshape(weights3_shape)\n",
    "biases3 = weights_and_biases[3428:3438]\n",
    "\n",
    "#Model Below\n",
    "model = Sequential()\n",
    "model.add(Conv2D(4, kernel_size=(3, 3), activation='relu', input_shape=(42, 42, 1)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(4, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation='relu'))\n",
    "\n",
    "model.layers[0].set_weights([weights, biases])\n",
    "model.layers[2].set_weights([weights2, biases2])\n",
    "model.layers[5].set_weights([weights3, biases3])\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(x_test_resized, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e90effb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
